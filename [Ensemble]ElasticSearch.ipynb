{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9db89ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data: 1: 100%|██████████| 4640/4640 [00:00<00:00, 39972.91it/s]\n",
      "data: 2: 100%|██████████| 4640/4640 [00:00<00:00, 840309.61it/s]\n",
      "data: 3: 100%|██████████| 4640/4640 [00:00<00:00, 744399.12it/s]\n",
      "data: 4: 100%|██████████| 4640/4640 [00:00<00:00, 796886.85it/s]\n",
      "data: 5: 100%|██████████| 4640/4640 [00:00<00:00, 793799.02it/s]\n",
      "data: 6: 100%|██████████| 4640/4640 [00:00<00:00, 779429.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'abs_my_summary.json', 'my_summary_n_tonull.json', 'abs_3_my_summary.json', '1923_tonull.json', 'best.json', '1923_del_summary.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# json list생성\n",
    "path_dir = \"./submissions/\"\n",
    "json_list = os.listdir(path_dir)\n",
    "json_list\n",
    "\n",
    "# json 불러서 summary data에 대한 dictionary 만들기\n",
    "summary_dict = {}\n",
    "\n",
    "for i, json_file in enumerate(json_list):\n",
    "    if json_file.endswith('.json'):\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "    with open(path_dir+json_file,\"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    for j,d in enumerate(tqdm(data, desc='data: '+str(i))):\n",
    "        try:\n",
    "            summary_dict[j].append(d['summary'])\n",
    "        except:\n",
    "            summary_dict[j] = [d['summary']]\n",
    "print(json_list)\n",
    "#dictionary 중복 제거\n",
    "for i in range(len(summary_dict)):\n",
    "    summary_dict[i] = list(set(summary_dict[i]))\n",
    "    #print(len(summary_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc615d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"filter\": {\n",
    "                    \"my_shingle\":{\n",
    "                        \"type\":\"shingle\"\n",
    "                    },\n",
    "                    \"my_stemmer\":{\n",
    "                        \"type\": \"stemmer\"\n",
    "                    }\n",
    "\n",
    "                },\n",
    "                \"analyzer\": {\n",
    "                    \"nori_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"nori_tokenizer\",\n",
    "                        \"decompound_mode\": \"mixed\",\n",
    "                        \"filter\": [\"my_shingle\",\"my_stemmer\"],\n",
    "\n",
    "                    },\n",
    "\n",
    "                    \n",
    "                },\n",
    "                \"similarity\": {\"my_similarity\": {\"type\": \"BM25\"}}        }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"dynamic\": \"strict\",\n",
    "            \"properties\": {\"document_text\": {\"type\": \"text\", \"analyzer\": \"nori_analyzer\"}}    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb67b91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4640/4640 [24:42<00:00,  3.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Elasticsearch index 생성\n",
    "# ! curl -X PUT localhost:9200/_cluster/settings -H \"Content-Type: application/json\" -d '{ \"persistent\": { \"cluster.max_shards_per_node\": \"10000\" } }'\n",
    "import argparse\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # default: show warnings\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    elasticsearch를 실행하기 전에 본 파일을 먼저 실행시켜주세요!\n",
    "    data가 변경되지 않았을 경우 실행하지 않아도 됩니다.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def set_index_and_server(index_name):\n",
    "    # Connect to elasticsearch\n",
    "    config = {\"host\": \"localhost\", \"port\": 9200}\n",
    "    es = Elasticsearch([config], timeout=30)\n",
    "    #print(\"Ping Elasticsearch :\", es.ping())\n",
    "\n",
    "    # Make index\n",
    "    if es.indices.exists(index_name):\n",
    "        #print(\"Index already exists. Creating a new one after deleting it...\")\n",
    "        pass\n",
    "        #es.indices.delete(index=index_name)\n",
    "    else:   \n",
    "        es.indices.create(index=index_name, body=body)\n",
    "    #print(\"Index creation has been completed\")\n",
    "\n",
    "    return es\n",
    "\n",
    "\n",
    "def insert_wiki_data(es, index_name):\n",
    "    # Load wiki data\n",
    "    wiki_articles = load_answer_data(index_name)\n",
    "\n",
    "    # Inserting wiki data\n",
    "    for i, rec in enumerate(wiki_articles):\n",
    "        try:\n",
    "            es.index(index=index_name, id=i, body=rec)\n",
    "        except:\n",
    "            print(f\"Unable to load document {i}.\")\n",
    "\n",
    "    # Show and count data\n",
    "    # sample_doc = es.get(index=index_name,id=1)\n",
    "    # print(sample_doc)\n",
    "    n_records = es.count(index=index_name)[\"count\"]\n",
    "    #print(f\"Succesfully loaded {n_records} into {index_name}\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    specials = {'\\n': '', '\\u2009': ' ', '\\u3000': ' ', '…': '...', '\\u200b': ' ', '(’': '(', '(‘': '('}\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])#re.sub(s, specials[s], text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_answer_data(index_name):\n",
    "    wiki_contexts = summary_dict[index_name]\n",
    "    wiki_contexts = [preprocess(text) for text in wiki_contexts]\n",
    "    wiki_articles = [\n",
    "        {\"document_text\": wiki_contexts[i]} for i in range(len(wiki_contexts))\n",
    "    ]\n",
    "\n",
    "    return wiki_articles\n",
    "\n",
    "\n",
    "def main(index_name):\n",
    "    #print(\"Start to Set Elastic Search\")\n",
    "    es = set_index_and_server(index_name)\n",
    "    insert_wiki_data(es=es, index_name=index_name)\n",
    "    #print(\"Finish\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for idx in tqdm(range(len(summary_dict))):\n",
    "        index_name = idx\n",
    "        main(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d0b357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26edbd8abdd9427784e6e479ea508810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Tuple, NoReturn, Any, Optional, Union\n",
    "\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    ")\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    #print(f\"[{name}] done in {time.time() - t0:.3f} s\")\n",
    "\n",
    "\n",
    "def elastic_setting(index_name=\"origin-wiki-index\"):\n",
    "    config = {\"host\": \"localhost\", \"port\": 9200}\n",
    "    es = Elasticsearch([config])\n",
    "    #print(\"elastic serach ping :\", es.ping())\n",
    "\n",
    "    return es, index_name\n",
    "\n",
    "\n",
    "def search_es(es, index_name, question_text, topk):\n",
    "    # index: index to search, body: query to search\n",
    "    query = {\"query\": {\"match\": {\"document_text\": question_text}}}\n",
    "    res = es.search(index=index_name, body=query, size=topk)  # size: default 10, top k\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "class SparseRetrieval:\n",
    "    def __init__(self,index_name) -> NoReturn:\n",
    "\n",
    "        # run_elastic_search.py를 먼저 실행시켜야합니다. 처음 한 번이면 될 것입니다!\n",
    "        self.es, self.index_name = elastic_setting(index_name=index_name)\n",
    "\n",
    "        # 삽입된 문서 1개 확인(es 결과 확인)\n",
    "        # print(self.es.get(index=self.index_name, id=1))\n",
    "\n",
    "    def retrieve_ES(\n",
    "        self, query_or_dataset: Union[str, Dataset], topk: Optional[int] = 1\n",
    "    ) -> Union[Tuple[List, List], pd.DataFrame]:\n",
    "\n",
    "        # Retrieve한 Passage를 pd.DataFrame으로 반환합니다.\n",
    "        total = []\n",
    "        with timer(\"query exhaustive search\"):\n",
    "            doc_scores, doc_indices, doc = self.get_relevant_doc_bulk_ES(\n",
    "                query_or_dataset[\"question\"], topk=topk\n",
    "            )\n",
    "\n",
    "        # 본 task에서 query는 1개이다.\n",
    "        topK_context = \"\"\n",
    "        for i, find_doc_idx in enumerate(doc):\n",
    "            if find_doc_idx != []:\n",
    "                topK_context = doc[i][0][\"_source\"][\"document_text\"]\n",
    "\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "\n",
    "            \"question\": query_or_dataset[\"question\"],\n",
    "            \"context\": topK_context,\n",
    "        }\n",
    "        total.append(tmp)\n",
    "\n",
    "        cqas = pd.DataFrame(total)\n",
    "        return cqas\n",
    "\n",
    "    def get_relevant_doc_bulk_ES(\n",
    "        self, queries: List, topk: Optional[int] = 1\n",
    "    ) -> Tuple[List, List]:\n",
    "\n",
    "        doc = []\n",
    "        doc_scores = []\n",
    "        doc_indices = []\n",
    "\n",
    "        for question in queries:\n",
    "\n",
    "            documents = search_es(self.es, self.index_name, question, topk)\n",
    "            doc.append(documents[\"hits\"][\"hits\"])\n",
    "\n",
    "            doc_score = []\n",
    "            doc_indice = []\n",
    "\n",
    "            for hit in documents[\"hits\"][\"hits\"]:\n",
    "                doc_score.append(hit[\"_score\"])\n",
    "                doc_indice.append(hit[\"_id\"])\n",
    "\n",
    "            doc_scores.append(doc_score)\n",
    "            doc_indices.append(doc_indice)\n",
    "\n",
    "        return doc_scores, doc_indices, doc\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # query 생성\n",
    "    # query를 doc_name으로 생성한다.\n",
    "    full_ds = {}\n",
    "    '''\n",
    "    full_ds = {0:{question:질문}}\n",
    "    '''\n",
    "    regex = r'\\([^)]*\\)'\n",
    "    for i,d in enumerate(data):\n",
    "        d['Meta']['doc_name'] = re.sub(r'\\([^)]*\\)', '', d['Meta']['doc_name'])\n",
    "        full_ds[i] = {'question':d['Meta']['doc_name']}\n",
    "        \n",
    "    # 새로운 df 정의\n",
    "    new_df = pd.DataFrame(\n",
    "        [], columns=[\"question\",\"context\"]\n",
    "    )\n",
    "    # 모든 index에 대해서 top1 뽑는다\n",
    "    for index_name in tqdm(range(len(summary_dict))):\n",
    "        retriever = SparseRetrieval(index_name)\n",
    "        result_retriever = retriever.retrieve_ES(full_ds[index_name], topk=1)\n",
    "        new_df = new_df.append(result_retriever,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a026c415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>무령왕릉 청동거울 일괄</td>\n",
       "      <td>공주시 무령왕릉에서 발견된 청동거울로 청동신수경, 의자손수대경, 수대경 3점이다. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>백자 대병</td>\n",
       "      <td>높이 47cm의 대형 백자 병으로 목이 유난히 길어 속칭 거위병이라고도 부른다. 긴...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>무령왕비 금귀걸이</td>\n",
       "      <td>굵은 고리를 중심으로 작은 장식들을 연결하여 만든 것이 백제 때 귀고리 2쌍으로 길...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>영월보덕사해우소</td>\n",
       "      <td>해우소는 '근심을 해결하는 장소' 라는 뜻의 사찰에서 화장실을 이르는 말인데, 해우...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>완도신흥사목조약사여래좌상</td>\n",
       "      <td>완도군 완도읍 군내리 신흥사에 모셔진 약사여래좌상은 원래 해남 대흥사 소속암자인 심...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4635</th>\n",
       "      <td>201221  한국전쟁 전후 민간인 희생자 위령시설 국제설계공모 당선작 발표.hwp</td>\n",
       "      <td>한국전쟁 전후 민간인 희생자 위령시설을 건립하기 위한 국제설계 공모에서 SGHS 설...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4636</th>\n",
       "      <td>201228 2021년 해양수산 분야 이렇게 달라집니다.hwp</td>\n",
       "      <td>국적선사는 2021년 긴급한 화물 수요가 있는 항로를 중심으로 상대적으로 취약한 중...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4637</th>\n",
       "      <td>201127 중대본회의 보도자료.hwp</td>\n",
       "      <td>정부지방자치단체 지시로 지시로 병상을 비웠으나 환자 치료에 사용하지 못한 병상 손실...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4638</th>\n",
       "      <td>7.9  신소재식품과.hwp</td>\n",
       "      <td>식품청과 농촌진흥청은 7월 9일 고단백 식품으로 탄수화물, 지방, 단백질 등 3대 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4639</th>\n",
       "      <td>200625전국 172곳 68022호 공공주택 입주자를 모집합니다.hwp</td>\n",
       "      <td>신혼희망타운으로 육아특화시설뿐만 아니라 스마트홈 기술, 통학길 특화, 층간소음 저감...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4640 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                                      무령왕릉 청동거울 일괄    \n",
       "1                                             백자 대병    \n",
       "2                                         무령왕비 금귀걸이    \n",
       "3                                          영월보덕사해우소    \n",
       "4                                     완도신흥사목조약사여래좌상    \n",
       "...                                              ...   \n",
       "4635  201221  한국전쟁 전후 민간인 희생자 위령시설 국제설계공모 당선작 발표.hwp   \n",
       "4636              201228 2021년 해양수산 분야 이렇게 달라집니다.hwp   \n",
       "4637                           201127 중대본회의 보도자료.hwp   \n",
       "4638                                 7.9  신소재식품과.hwp   \n",
       "4639        200625전국 172곳 68022호 공공주택 입주자를 모집합니다.hwp   \n",
       "\n",
       "                                                context  \n",
       "0     공주시 무령왕릉에서 발견된 청동거울로 청동신수경, 의자손수대경, 수대경 3점이다. ...  \n",
       "1     높이 47cm의 대형 백자 병으로 목이 유난히 길어 속칭 거위병이라고도 부른다. 긴...  \n",
       "2     굵은 고리를 중심으로 작은 장식들을 연결하여 만든 것이 백제 때 귀고리 2쌍으로 길...  \n",
       "3     해우소는 '근심을 해결하는 장소' 라는 뜻의 사찰에서 화장실을 이르는 말인데, 해우...  \n",
       "4     완도군 완도읍 군내리 신흥사에 모셔진 약사여래좌상은 원래 해남 대흥사 소속암자인 심...  \n",
       "...                                                 ...  \n",
       "4635  한국전쟁 전후 민간인 희생자 위령시설을 건립하기 위한 국제설계 공모에서 SGHS 설...  \n",
       "4636  국적선사는 2021년 긴급한 화물 수요가 있는 항로를 중심으로 상대적으로 취약한 중...  \n",
       "4637  정부지방자치단체 지시로 지시로 병상을 비웠으나 환자 치료에 사용하지 못한 병상 손실...  \n",
       "4638  식품청과 농촌진흥청은 7월 9일 고단백 식품으로 탄수화물, 지방, 단백질 등 3대 ...  \n",
       "4639  신혼희망타운으로 육아특화시설뿐만 아니라 스마트홈 기술, 통학길 특화, 층간소음 저감...  \n",
       "\n",
       "[4640 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a181db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_dir+json_list[4],\"r\") as f:\n",
    "    best_data = json.load(f)\n",
    "\n",
    "with open('test_summary.json', encoding='utf-8') as file:\n",
    "    test_dataset = json.load(file)\n",
    "    \n",
    "for j in range(len(new_df)):\n",
    "    if new_df['context'][j]=='':\n",
    "        new_df['context'][j] = best_data[j]['summary']\n",
    "\n",
    "for j in range(len(new_df)):\n",
    "    if new_df['context'][j]=='':\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93a1f786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4640\n"
     ]
    }
   ],
   "source": [
    "summary_list = list(new_df['context'])\n",
    "print(len(summary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49012314",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(summary_list)):\n",
    "    test_dataset[i]['summary'] = summary_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6bc282cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': '공주시 무령왕릉에서 발견된 청동거울로 청동신수경, 의자손수대경, 수대경 3점이다. 청동신수경은 ‘방격규구문경’이라는 중국 후한의 거울을 모방하여 만든 것이다. 거울 내부에는 반나체 인물상과 글이 새겨져 있는데 이는 한나라의 거울에서 흔히 볼 수 있는 것이다. 의자손수대경은 중국 한대의 수대경을 본떠 만든 방제경이다. 거울 중앙의 꼭지를 중심으로 9개의 돌기가 있고, 안에는 크고 작은 원과 7개의 돌기가 솟아있다. 내부 주위의 테두리에는 명문이 새겨져 있으나 선명하지 못하여 알아볼 수 없다. 수대경 역시 한나라 때 동물 문양을 새겨 넣은 수대경을 본떠서 만들어진 방제경이다. 그러나 한나라 거울에 비해 선이 굵고 무늬가 정교하지 못하다.',\n",
       " 'summary': '공주시 무령왕릉에서 발견된 청동거울로 청동신수경, 의자손수대경, 수대경 3점이다. 청동신수경은 ‘방격규구문경’이라는 중국 후한의 거울을 모방하여 만든 것이다. 수대경 역시 한나라 때 동물 문양을 새겨 넣은 수대경을 본떠서 만들어진 방제경이다.',\n",
       " 'Meta': {'passage_id': 'REPORT-cultural_assets-00164-01180',\n",
       "  'doc_name': '무령왕릉 청동거울 일괄 (武寧王陵 銅鏡 一括)',\n",
       "  'category': 'cul_ass',\n",
       "  'author': None,\n",
       "  'publisher': None,\n",
       "  'publisher_year': None,\n",
       "  'doc_origin': '문화재청'}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8831488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('elastic_del_b.json', 'w', encoding=\"utf-8\") as file:\n",
    "    json.dump(test_dataset, file, indent='\\t', ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982623da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
