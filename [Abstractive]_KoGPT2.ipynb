{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uu4bxUOP5BVB",
    "outputId": "63c27bbb-3069-4b4f-aa48-4227126aae9f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: kogpt2_transformers in /opt/conda/envs/lightweight/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: transformers>=4.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from kogpt2_transformers) (4.12.5)\n",
      "Requirement already satisfied: torch>=1.1.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from kogpt2_transformers) (1.6.0)\n",
      "Requirement already satisfied: tokenizers>=0.7.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from kogpt2_transformers) (0.10.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from torch>=1.1.0->kogpt2_transformers) (1.21.4)\n",
      "Requirement already satisfied: future in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from torch>=1.1.0->kogpt2_transformers) (0.18.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers>=4.0.0->kogpt2_transformers) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers>=4.0.0->kogpt2_transformers) (2021.11.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers>=4.0.0->kogpt2_transformers) (0.1.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers>=4.0.0->kogpt2_transformers) (4.62.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers>=4.0.0->kogpt2_transformers) (0.0.46)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers>=4.0.0->kogpt2_transformers) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers>=4.0.0->kogpt2_transformers) (4.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers>=4.0.0->kogpt2_transformers) (21.2)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers>=4.0.0->kogpt2_transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.0.0->kogpt2_transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from packaging>=20.0->transformers>=4.0.0->kogpt2_transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.0.0->kogpt2_transformers) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests->transformers>=4.0.0->kogpt2_transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests->transformers>=4.0.0->kogpt2_transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests->transformers>=4.0.0->kogpt2_transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests->transformers>=4.0.0->kogpt2_transformers) (1.26.7)\n",
      "Requirement already satisfied: click in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from sacremoses->transformers>=4.0.0->kogpt2_transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from sacremoses->transformers>=4.0.0->kogpt2_transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from sacremoses->transformers>=4.0.0->kogpt2_transformers) (1.16.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: wandb in /opt/conda/envs/lightweight/lib/python3.7/site-packages (0.12.7)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: pathtools in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: yaspin>=1.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (2.1.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (3.1.24)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from wandb) (5.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.8.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.6.0)\r\n"
     ]
    }
   ],
   "source": [
    "# 필요한 패키지를 설치\n",
    "!pip install kogpt2_transformers\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w4-AtAS_5N4f"
   },
   "outputs": [],
   "source": [
    "# util\n",
    "from kogpt2_transformers import get_kogpt2_tokenizer\n",
    "import json\n",
    "\n",
    "def json_load(data_path = '/opt/ml/data/요약대회/train_summary.json'):\n",
    "    data = []\n",
    "    with open(data_path, 'r') as json_file:\n",
    "            json_list = json.load(json_file)\n",
    "            \n",
    "    return json_list\n",
    "\n",
    "\n",
    "def token_num(data_path = '/opt/ml/data/요약대회/train_summary.json'):\n",
    "    data = []\n",
    "    with open(data_path, 'r') as json_file:\n",
    "            json_list = json.load(json_file)\n",
    "\n",
    "    gpt_tok = get_kogpt2_tokenizer()\n",
    "    gpt_tok_num = 0\n",
    "    count = 0\n",
    "\n",
    "    for json_str in json_list:\n",
    "            tmp_str = json_str['summary']\n",
    "            gpt_tok_num = max(gpt_tok_num, len(gpt_tok.encode(tmp_str, max_length=512, truncation=True)))\n",
    "\n",
    "    print('max gpt token len:', gpt_tok_num)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     token_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6sEaOTpF46bb"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from kogpt2_transformers import get_kogpt2_model\n",
    "\n",
    "\n",
    "class AbstractiveKoGPT2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AbstractiveKoGPT2, self).__init__()\n",
    "        self.kogpt2 = get_kogpt2_model()\n",
    "\n",
    "    def generate(self,\n",
    "                 input_ids,\n",
    "                 do_sample=True,\n",
    "                 max_length= 60,\n",
    "                 top_p=0.92,\n",
    "                 top_k=50,\n",
    "                 temperature= 0.6,\n",
    "                 no_repeat_ngram_size =None,\n",
    "                 num_return_sequences=3,\n",
    "                 early_stopping=False,\n",
    "                 ):\n",
    "        return self.kogpt2.generate(input_ids,\n",
    "                                     do_sample=do_sample,\n",
    "                                     max_length=max_length,\n",
    "                                     top_p = top_p, # 글의 표현 범위 조절\n",
    "                                     top_k=top_k, # 글의 표현 범위 조절\n",
    "                                     temperature=temperature, # 글의 창의성 조절\n",
    "                                     no_repeat_ngram_size= no_repeat_ngram_size,\n",
    "                                     num_return_sequences=num_return_sequences,\n",
    "                                     early_stopping = early_stopping,\n",
    "                                     eos_token_id = 1,\n",
    "                                     pad_token_id= 3\n",
    "                                    )\n",
    "\n",
    "    def forward(self, input, labels = None):\n",
    "        if labels is not None:\n",
    "            outputs = self.kogpt2(input, labels=labels)\n",
    "        else:\n",
    "            outputs = self.kogpt2(input)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cMa6um9HTppr"
   },
   "outputs": [],
   "source": [
    "# dataset \n",
    "class AbstrativeDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 n_ctx = 1024, \n",
    "                 articles_max_length = 810,\n",
    "                 summary_max_length = 210,\n",
    "                 device = 'cpu'\n",
    "                 ):\n",
    "        self.data =[]\n",
    "        self.device = device\n",
    "        self.tokenizer = get_kogpt2_tokenizer()\n",
    "\n",
    "        bos_token_id = [self.tokenizer.bos_token_id] # <s>, 0\n",
    "        eos_token_id = [self.tokenizer.eos_token_id] # </s>, 1\n",
    "        pad_token_id = [self.tokenizer.pad_token_id] # <pad>, 3\n",
    "\n",
    "        json_datas = json_load()\n",
    "\n",
    "        for dict_data in tqdm(json_datas):\n",
    "            articles = dict_data['original']\n",
    "            abstractive_summary = dict_data['summary']\n",
    "\n",
    "#             tmp_str =''\n",
    "#             for article in articles:\n",
    "#                 tmp_str += article\n",
    "\n",
    "            # encode\n",
    "            # truncate, if string exceed max length\n",
    "            enc_tmp_str = self.tokenizer.encode(articles, truncation= True, max_length=articles_max_length)\n",
    "            enc_abstractive_summary = self.tokenizer.encode(abstractive_summary, truncation= True, max_length=summary_max_length)\n",
    "            \n",
    "            # <s> 요약할 문장 </s> 요약된 문장 </s>\n",
    "            index_of_words = bos_token_id + enc_tmp_str+ eos_token_id + enc_abstractive_summary + eos_token_id\n",
    "            pad_token_len = n_ctx - len(index_of_words)\n",
    "            index_of_words += pad_token_id * pad_token_len\n",
    "            print(f'max: {max(index_of_words)}, min: {min(index_of_words)}')\n",
    "            \n",
    "            # RuntimeError: CUDA error: device-side assert triggered\n",
    "            if max(index_of_words)<50000:\n",
    "                self.data.append(torch.tensor(index_of_words).to(device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        item = self.data[index]\n",
    "        return item\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     dataset = AbstrativeDataset()\n",
    "#     print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "cvv5THuiTprE",
    "outputId": "3f7f7f54-0953-4f16-a1fa-c0fd497f7842"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from torch.utils.data import dataloader\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "def set_seed(seed = 42):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    set_seed()\n",
    "    wandb.init(project= 'bakbak', entity= 'quarter100', name= f'KG')\n",
    "    checkpoint_path =\"/opt/ml/data/요약대회/checkpoint\"\n",
    "    save_ckpt_path = f\"{checkpoint_path}/kogpt2-abstractive.pth\"\n",
    "\n",
    "    n_epoch = 5         # Num of Epoch\n",
    "    batch_size = 4      # 배치 사이즈\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    #device = 'cpu'\n",
    "    print('device: ',device)\n",
    "    save_step = 100 # 학습 저장 주기\n",
    "    learning_rate = 5e-5  # Learning Rate\n",
    "\n",
    "    dataset= AbstrativeDataset(device=device)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = AbstractiveKoGPT2()\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=3)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_losses = []\n",
    "    losses =[]\n",
    "\n",
    "    if os.path.isfile(save_ckpt_path):\n",
    "        checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
    "        pre_epoch = checkpoint['epoch']\n",
    "        pre_loss = checkpoint['loss']\n",
    "        total_losses = checkpoint['losses']\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}, loss={pre_loss}\")\n",
    "        \n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        count = 0\n",
    "        with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
    "            for i, data in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "#                 for d in data:\n",
    "#                     print(max(d),min(d))\n",
    "                outputs = model(data, labels=data)\n",
    "                _, logits = outputs[:2]\n",
    "\n",
    "                # Shift so that tokens < n predict n\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = data[..., 1:].contiguous()\n",
    "\n",
    "                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                if (count > 0 and count % save_step == 0) or (len(data) < batch_size):\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'train_no': count,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        'losses': losses\n",
    "                    }, save_ckpt_path)\n",
    "                count += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")\n",
    "                if i%50==0:\n",
    "                    wandb.log({'loss':np.mean(losses)})\n",
    "\n",
    "        total_losses.append(np.mean(losses))\n",
    "\n",
    "    # data\n",
    "    data = {\n",
    "        \"loss\": total_losses\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    display(df)\n",
    "\n",
    "    # graph\n",
    "    plt.figure(figsize=[12, 4])\n",
    "    plt.plot(losses, label=\"loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalAbstrativeDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 tokenizer,\n",
    "                 n_ctx = 1024,\n",
    "                 data_path='/opt/ml/data/요약대회/test_summary.json',\n",
    "                 articles_max_length = 810,\n",
    "                 summary_max_length = 210,\n",
    "                 ):\n",
    "        self.data =[]\n",
    "        self.tokenizer = tokenizer\n",
    "        print(device)\n",
    "        \n",
    "        bos_token_id = [self.tokenizer.bos_token_id] # <s>\n",
    "        eos_token_id = [self.tokenizer.eos_token_id] # </s>\n",
    "\n",
    "        json_datas = json_load(data_path=data_path)\n",
    "        \n",
    "        for dict_data in tqdm(json_datas):\n",
    "            #id = dict_data['id']\n",
    "            articles = dict_data['original']\n",
    "\n",
    "            tmp_str =''\n",
    "            for article in articles:\n",
    "                tmp_str += article\n",
    "\n",
    "            # encode\n",
    "            # truncate, if string exceed max length\n",
    "            enc_tmp_str = self.tokenizer.encode(tmp_str, truncation= True, max_length=articles_max_length)\n",
    "\n",
    "            # <s> 요약할 문장 </s> 요약된 문장 </s>\n",
    "            index_of_words = bos_token_id + enc_tmp_str+ eos_token_id\n",
    "\n",
    "            self.data.append({\n",
    "                #'id':id,\n",
    "                'input':torch.tensor([index_of_words]).to(device)\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        item = self.data[index]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4640/4640 [00:07<00:00, 646.64it/s]\n",
      "100%|██████████| 1303/1303 [32:39<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "tokenizer = get_kogpt2_tokenizer()\n",
    "ckpt_path = '/opt/ml/data/요약대회/checkpoint/kogpt2-abstractive.pth'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "eval_datas = EvalAbstrativeDataset(tokenizer=tokenizer,device=device,data_path='/opt/ml/data/요약대회/test_summary.json')\n",
    "\n",
    "checkpoint = torch.load(ckpt_path,map_location=torch.device(device))\n",
    "model = AbstractiveKoGPT2()\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "my_summaries=[]\n",
    "# for data in tqdm(eval_datas[:3337]):\n",
    "#     input_ids = data['input']\n",
    "#     sample_output = model.generate(input_ids=input_ids,max_length=1024)\n",
    "#     summary = tokenizer.decode(sample_output[0].tolist()[len(input_ids[0]):-1])\n",
    "#     summary = summary.replace('</s>','').replace('<pad>','')\n",
    "#     my_summaries.append(summary)\n",
    "\n",
    "for data in tqdm(eval_datas[3337:]):\n",
    "    input_ids = data['input']\n",
    "    max_toc = max(input_ids[0])\n",
    "    min_toc = min(input_ids[0])\n",
    "    if max_toc<50000:\n",
    "        sample_output = model.generate(input_ids=input_ids,max_length=1024)\n",
    "        summary = tokenizer.decode(sample_output[0].tolist()[len(input_ids[0]):-1])\n",
    "        summary = summary.replace('</s>','').replace('<pad>','')\n",
    "        my_summaries.append(summary)\n",
    "    else:\n",
    "        summary = tokenizer.decode(input_ids[0])\n",
    "        summary = summary.replace('<s>','').replace('</s>','').replace('<pad>','')\n",
    "        my_summaries.append(summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배열 list작성\n",
    "with open('my_summaries_kg_base.json', 'w', encoding=\"UTF-8\") as file:\n",
    "    json.dump(my_summaries, file, indent='\\t', ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "j3md8YsogCdS"
   },
   "outputs": [],
   "source": [
    "with open('my_summaries_kg_base.json', 'r', encoding=\"UTF-8\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4640"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 형식 담긴 파일\n",
    "with open('test_summary.json', 'r', encoding=\"UTF-8\") as file:\n",
    "    test_file = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test,summary in zip(test_file,data):\n",
    "    test['summary'] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출용 파일\n",
    "with open('submit_kg_base.json', 'w', encoding=\"UTF-8\") as file:\n",
    "    json.dump(test_file, file, indent='\\t', ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "요약대회",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
