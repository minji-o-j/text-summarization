{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a0436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simplet5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (0.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from simplet5) (4.62.3)\n",
      "Requirement already satisfied: pytorch-lightning==1.4.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from simplet5) (1.4.5)\n",
      "Requirement already satisfied: transformers==4.10.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from simplet5) (4.10.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from simplet5) (0.1.96)\n",
      "Requirement already satisfied: torch!=1.8.0,>=1.7.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from simplet5) (1.10.0+cu113)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.4.5->simplet5) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.4.5->simplet5) (1.21.4)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.4.5->simplet5) (0.18.2)\n",
      "Requirement already satisfied: torchmetrics>=0.4.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.4.5->simplet5) (0.6.0)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.4.5->simplet5) (0.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.4.5->simplet5) (21.2)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.4.5->simplet5) (2021.11.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.4.5->simplet5) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pytorch-lightning==1.4.5->simplet5) (3.10.0.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.10.0->simplet5) (4.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.10.0->simplet5) (0.1.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.10.0->simplet5) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.10.0->simplet5) (2021.11.10)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.10.0->simplet5) (0.0.46)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.10.0->simplet5) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from transformers==4.10.0->simplet5) (3.4.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.4.5->simplet5) (3.8.1)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from packaging>=17.0->pytorch-lightning==1.4.5->simplet5) (2.4.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (2.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (3.3.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (1.0.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (1.42.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (0.4.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (58.0.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (3.19.1)\n",
      "Requirement already satisfied: six in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from importlib-metadata->transformers==4.10.0->simplet5) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests->transformers==4.10.0->simplet5) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests->transformers==4.10.0->simplet5) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests->transformers==4.10.0->simplet5) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests->transformers==4.10.0->simplet5) (2.0.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.4.5->simplet5) (3.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.4.5->simplet5) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.4.5->simplet5) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.4.5->simplet5) (21.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.4.5->simplet5) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.4.5->simplet5) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.4.5->simplet5) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.4.5->simplet5) (5.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from sacremoses->transformers==4.10.0->simplet5) (8.0.3)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/lightweight/lib/python3.7/site-packages (from sacremoses->transformers==4.10.0->simplet5) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install simplet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09058769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a690ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json\n",
    "def json_load(data_path = '/opt/ml/data/요약대회/train_summary.json'):\n",
    "    data = []\n",
    "    with open(data_path, 'r') as json_file:\n",
    "            json_list = json.load(json_file)\n",
    "    return json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "376aaf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'original': '만정헌은 약 500여년전 현감벼슬을 지냈던 김자간이 경주에서 울산 명촌리에 들어와 자리를 잡으면서 세운 경주 김씨의 정각이다. 현재 건물은 약 200여년 전에 고쳐 지은 것으로 1칸이던 온돌방을 2칸으로 늘려 지었다. 규모는 앞면이 3칸인데 비해 옆면은 왼쪽이 3칸·오른쪽이 2칸으로 되어 있다. 지붕은 옆면에서 볼 때 여덟 팔(八)자 모양인 팔작지붕이다. 앞면에는 ‘명헌’이라는 현판이 걸려 있으며, 둥근기둥(두리기둥)에 싸리나무를 사용한 것이 특징이다. 약 500여년의 전통을 지닌 만정헌은 울산에서 가장 오래된 건축문화재로, 지붕 처마의 날렵하게 치켜 올라간 곡선이 한국 건축의 아름다움을 잘 나타내고 있다.',\n",
       "  'summary': '만정헌은 약 500여년전 현감벼슬을 지냈던 김자간이 경주에서 울산 명촌리에 들어와 자리를 잡으면서 세운 경주 김씨의 정각이다.  약 500여년의 전통을 지닌 만정헌은 울산에서 가장 오래된 건축문화재로, 지붕 처마의 날렵하게 치켜 올라간 곡선이 한국 건축의 아름다움을 잘 나타내고 있다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-10670-02593',\n",
       "   'doc_name': '만정헌 (晩定軒)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '부여 관북리유적은 1983년 9월 충청남도 기념물 제43호 전백제왕궁지(傳百濟王宮址)로 지정되어 있다가, 2001년 2월 사적으로 승격 지정되었다.1982년부터 충남대학교 박물관에서 5차에 걸쳐 발굴조사를 하였는데, 1983년도에는 방형석축연지(方形石築蓮池)가 발견되었고, 1988년 발굴조사에서는 토기 구연부에 북사(北舍)라는 명문이 발견되었으며, 1992년 조사에서는 현재 국립부여문화재연구소의 남쪽 50m 지점에서 백제시대의 도로유적과 배수시설이 드러났다.백제시대 마지막 도읍이었던 사비도성의 일부 유적이 밝혀진 것은 매우 중요한 학술적 의미를 지닌다고 할 수 있다.※(부여관북리백제유적 → 부여 관북리 유적)으로 명칭변경 되었습니다.(2011.07.28 고시)',\n",
       "  'summary': '부여 관북리유적은 1983년 9월 충청남도 기념물 제43호 전백제왕궁지(傳百濟王宮址)로 지정되어 있다가, 2001년 2월 사적으로 승격 지정되었다.  백제시대 마지막 도읍이었던 사비도성의 일부 유적이 밝혀진 것은 매우 중요한 학술적 의미를 지닌다고 할 수 있다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-03009-00980',\n",
       "   'doc_name': '부여 관북리 유적 (扶餘 官北里 遺蹟)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data 확인\n",
    "train_data = json_load()\n",
    "train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde2c45f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18548/18548 [00:48<00:00, 385.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# data to dataframe\n",
    "# df 생성, simpleT5는 \"source_text\", \"target_text\"로 이름 설정 필요함\n",
    "df = pd.DataFrame(columns = ['source_text', 'target_text']) # source_text: 원본, target_text: 요약된 문장\n",
    "\n",
    "# df에 데이터 추가\n",
    "for data in tqdm(train_data):\n",
    "    df.loc[len(df)] = ['summarize: '+data['original'],data['summary']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7939e6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summarize: 만정헌은 약 500여년전 현감벼슬을 지냈던 김자간이 경주에서 울...</td>\n",
       "      <td>만정헌은 약 500여년전 현감벼슬을 지냈던 김자간이 경주에서 울산 명촌리에 들어와 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summarize: 부여 관북리유적은 1983년 9월 충청남도 기념물 제43호 전백...</td>\n",
       "      <td>부여 관북리유적은 1983년 9월 충청남도 기념물 제43호 전백제왕궁지(傳百濟王宮址...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summarize: 『문정공 김상헌 진적』은 청음 김상헌 친필로 일부는 그의 손자인...</td>\n",
       "      <td>내용의 대부분이『청음집』에 수록 되어있으나, 본 자료에는 붉은색으로 교정을 가한 표...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summarize: 조선 후기 안동김씨 세도기 후반의 중심인물이었던 하옥 김좌근의 ...</td>\n",
       "      <td>조선 후기 안동김씨 세도기 후반의 중심인물이었던 하옥 김좌근의 아들인 김병기(金炳冀...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summarize: 외토리 마을에 있는 한 채의 비각 안에 나란히 서 있는 2기의 ...</td>\n",
       "      <td>외토리 마을에 있는 한 채의 비각 안에 나란히 서 있는 2기의 비이다.  앞에서 보...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18543</th>\n",
       "      <td>summarize: 이어서 소셜벤처를 위한 보증프로그램인 임팩트보증을 담당하고 있는...</td>\n",
       "      <td>특히 이날 간담회의 모든 내용은 소셜벤처인 소리를보는통로의 인공지능(AI) 기반 실...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18544</th>\n",
       "      <td>summarize: 아라온호, 유빙수역에 갇힌 원양어선 구조에 나섰다\\n 해양수산부...</td>\n",
       "      <td>해양수산부(장관 문성혁)는 2020. 1. 14.(화) 08시경(한국시간)부터 아라...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18545</th>\n",
       "      <td>summarize: 항만법‧항만 재개발법, 새단장 후 7월 30일 시행\\n항만법하위...</td>\n",
       "      <td>해양수산부(장관 문성혁)는 7월 21일(화) 국무회의에서 항만법 하위법령 전부개정안...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18546</th>\n",
       "      <td>summarize:  또한, 인공지능 활용능력 보유인재 양성 및 인재인공지능 간 지...</td>\n",
       "      <td>정부는 장기적으로 미래의 도전과제를 제시하고 고위험 대형연구에 집중투자하며, 기업이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18547</th>\n",
       "      <td>summarize:  현재 종이, 유리, 철에만 적용되던 재생원료 의무사용제도를 내...</td>\n",
       "      <td>현재 종이, 유리, 철에만 적용되던 재생원료 의무사용제도를 내년부터 단계적으로 플라...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18548 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             source_text  \\\n",
       "0      summarize: 만정헌은 약 500여년전 현감벼슬을 지냈던 김자간이 경주에서 울...   \n",
       "1      summarize: 부여 관북리유적은 1983년 9월 충청남도 기념물 제43호 전백...   \n",
       "2      summarize: 『문정공 김상헌 진적』은 청음 김상헌 친필로 일부는 그의 손자인...   \n",
       "3      summarize: 조선 후기 안동김씨 세도기 후반의 중심인물이었던 하옥 김좌근의 ...   \n",
       "4      summarize: 외토리 마을에 있는 한 채의 비각 안에 나란히 서 있는 2기의 ...   \n",
       "...                                                  ...   \n",
       "18543  summarize: 이어서 소셜벤처를 위한 보증프로그램인 임팩트보증을 담당하고 있는...   \n",
       "18544  summarize: 아라온호, 유빙수역에 갇힌 원양어선 구조에 나섰다\\n 해양수산부...   \n",
       "18545  summarize: 항만법‧항만 재개발법, 새단장 후 7월 30일 시행\\n항만법하위...   \n",
       "18546  summarize:  또한, 인공지능 활용능력 보유인재 양성 및 인재인공지능 간 지...   \n",
       "18547  summarize:  현재 종이, 유리, 철에만 적용되던 재생원료 의무사용제도를 내...   \n",
       "\n",
       "                                             target_text  \n",
       "0      만정헌은 약 500여년전 현감벼슬을 지냈던 김자간이 경주에서 울산 명촌리에 들어와 ...  \n",
       "1      부여 관북리유적은 1983년 9월 충청남도 기념물 제43호 전백제왕궁지(傳百濟王宮址...  \n",
       "2      내용의 대부분이『청음집』에 수록 되어있으나, 본 자료에는 붉은색으로 교정을 가한 표...  \n",
       "3      조선 후기 안동김씨 세도기 후반의 중심인물이었던 하옥 김좌근의 아들인 김병기(金炳冀...  \n",
       "4      외토리 마을에 있는 한 채의 비각 안에 나란히 서 있는 2기의 비이다.  앞에서 보...  \n",
       "...                                                  ...  \n",
       "18543  특히 이날 간담회의 모든 내용은 소셜벤처인 소리를보는통로의 인공지능(AI) 기반 실...  \n",
       "18544  해양수산부(장관 문성혁)는 2020. 1. 14.(화) 08시경(한국시간)부터 아라...  \n",
       "18545  해양수산부(장관 문성혁)는 7월 21일(화) 국무회의에서 항만법 하위법령 전부개정안...  \n",
       "18546  정부는 장기적으로 미래의 도전과제를 제시하고 고위험 대형연구에 집중투자하며, 기업이...  \n",
       "18547  현재 종이, 유리, 철에만 적용되던 재생원료 의무사용제도를 내년부터 단계적으로 플라...  \n",
       "\n",
       "[18548 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2fb9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77813b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# SimpleT5\n",
    "# https://github.com/Shivanandroy/simpleT5 , logging 추가\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    MT5ForConditionalGeneration,\n",
    "    ByT5Tokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    T5TokenizerFast as T5Tokenizer,\n",
    "    MT5TokenizerFast as MT5Tokenizer,\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# from fastT5 import export_and_get_onnx_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "pl.seed_everything(42)\n",
    "\n",
    "\n",
    "class PyTorchDataModule(Dataset):\n",
    "    \"\"\"  PyTorch Dataset class  \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        source_max_token_len: int = 512,\n",
    "        target_max_token_len: int = 512,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        initiates a PyTorch Dataset Module for input data\n",
    "        Args:\n",
    "            data (pd.DataFrame): input pandas dataframe. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n",
    "            tokenizer (PreTrainedTokenizer): a PreTrainedTokenizer (T5Tokenizer, MT5Tokenizer, or ByT5Tokenizer)\n",
    "            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n",
    "            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" returns length of data \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\" returns dictionary of input tensors to feed into T5/MT5 model\"\"\"\n",
    "\n",
    "        data_row = self.data.iloc[index]\n",
    "        source_text = data_row[\"source_text\"]\n",
    "\n",
    "        source_text_encoding = self.tokenizer(\n",
    "            source_text,\n",
    "            max_length=self.source_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        target_text_encoding = self.tokenizer(\n",
    "            data_row[\"target_text\"],\n",
    "            max_length=self.target_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = target_text_encoding[\"input_ids\"]\n",
    "        labels[\n",
    "            labels == 0\n",
    "        ] = -100  # to make sure we have correct labels for T5 text generation\n",
    "\n",
    "        return dict(\n",
    "            source_text=source_text,\n",
    "            target_text=data_row[\"target_text\"],\n",
    "            source_text_input_ids=source_text_encoding[\"input_ids\"].flatten(),\n",
    "            source_text_attention_mask=source_text_encoding[\"attention_mask\"].flatten(),\n",
    "            labels=labels.flatten(),\n",
    "            labels_attention_mask=target_text_encoding[\"attention_mask\"].flatten(),\n",
    "        )\n",
    "\n",
    "\n",
    "class LightningDataModule(pl.LightningDataModule):\n",
    "    \"\"\" PyTorch Lightning data class \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        batch_size: int = 4,\n",
    "        source_max_token_len: int = 512,\n",
    "        target_max_token_len: int = 512,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        initiates a PyTorch Lightning Data Module\n",
    "        Args:\n",
    "            train_df (pd.DataFrame): training dataframe. Dataframe must contain 2 columns --> \"source_text\" & \"target_text\"\n",
    "            test_df (pd.DataFrame): validation dataframe. Dataframe must contain 2 columns --> \"source_text\" & \"target_text\"\n",
    "            tokenizer (PreTrainedTokenizer): PreTrainedTokenizer (T5Tokenizer, MT5Tokenizer, or ByT5Tokenizer)\n",
    "            batch_size (int, optional): batch size. Defaults to 4.\n",
    "            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n",
    "            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = PyTorchDataModule(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len,\n",
    "        )\n",
    "        self.test_dataset = PyTorchDataModule(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\" training dataloader \"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\" test dataloader \"\"\"\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\" validation dataloader \"\"\"\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2\n",
    "        )\n",
    "\n",
    "\n",
    "class LightningModel(pl.LightningModule):\n",
    "    \"\"\" PyTorch Lightning Model class\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, model, outputdir: str = \"outputs\"):\n",
    "        \"\"\"\n",
    "        initiates a PyTorch Lightning Model\n",
    "        Args:\n",
    "            tokenizer : T5/MT5/ByT5 tokenizer\n",
    "            model : T5/MT5/ByT5 model\n",
    "            outputdir (str, optional): output directory to save model checkpoints. Defaults to \"outputs\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.outputdir = outputdir\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n",
    "        \"\"\" forward step \"\"\"\n",
    "        output = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        return output.loss, output.logits\n",
    "\n",
    "    def training_step(self, batch, batch_size):\n",
    "        \"\"\" training step \"\"\"\n",
    "        input_ids = batch[\"source_text_input_ids\"]\n",
    "        attention_mask = batch[\"source_text_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
    "\n",
    "        loss, outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=labels_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_size):\n",
    "        \"\"\" validation step \"\"\"\n",
    "        input_ids = batch[\"source_text_input_ids\"]\n",
    "        attention_mask = batch[\"source_text_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
    "\n",
    "        loss, outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=labels_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_size):\n",
    "        \"\"\" test step \"\"\"\n",
    "        input_ids = batch[\"source_text_input_ids\"]\n",
    "        attention_mask = batch[\"source_text_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
    "\n",
    "        loss, outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=labels_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" configure optimizers \"\"\"\n",
    "        return AdamW(self.parameters(), lr=0.0001)\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        \"\"\" save tokenizer and model on epoch end \"\"\"\n",
    "        avg_traning_loss = np.round(\n",
    "            torch.mean(torch.stack([x[\"loss\"] for x in training_step_outputs])).item(),\n",
    "            4,\n",
    "        )\n",
    "        path = f\"{self.outputdir}/simplet5-epoch-{self.current_epoch}-train-loss-{str(avg_traning_loss)}\"\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        self.model.save_pretrained(path)\n",
    "\n",
    "    # def validation_epoch_end(self, validation_step_outputs):\n",
    "    #     # val_loss = torch.stack([x['loss'] for x in validation_step_outputs]).mean()\n",
    "    #     path = f\"{self.outputdir}/T5-epoch-{self.current_epoch}\"\n",
    "    #     self.tokenizer.save_pretrained(path)\n",
    "    #     # self.model.save_pretrained(path)\n",
    "\n",
    "\n",
    "class SimpleT5:\n",
    "    \"\"\" Custom SimpleT5 class \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\" initiates SimpleT5 class \"\"\"\n",
    "        pass\n",
    "\n",
    "    def from_pretrained(self, model_type=\"t5\", model_name=\"t5-base\") -> None:\n",
    "        \"\"\"\n",
    "        loads T5/MT5 Model model for training/finetuning\n",
    "        Args:\n",
    "            model_type (str, optional): \"t5\" or \"mt5\" . Defaults to \"t5\".\n",
    "            model_name (str, optional): exact model architecture name, \"t5-base\" or \"t5-large\". Defaults to \"t5-base\".\n",
    "        \"\"\"\n",
    "        if model_type == \"t5\":\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(f\"{model_name}\")\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                f\"{model_name}\", return_dict=True\n",
    "            )\n",
    "        elif model_type == \"mt5\":\n",
    "            self.tokenizer = MT5Tokenizer.from_pretrained(f\"{model_name}\")\n",
    "            self.model = MT5ForConditionalGeneration.from_pretrained(\n",
    "                f\"{model_name}\", return_dict=True\n",
    "            )\n",
    "        elif model_type == \"byt5\":\n",
    "            self.tokenizer = ByT5Tokenizer.from_pretrained(f\"{model_name}\")\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                f\"{model_name}\", return_dict=True\n",
    "            )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        eval_df: pd.DataFrame,\n",
    "        source_max_token_len: int = 512,\n",
    "        target_max_token_len: int = 512,\n",
    "        batch_size: int = 8,\n",
    "        max_epochs: int = 5,\n",
    "        use_gpu: bool = True,\n",
    "        outputdir: str = \"outputs\",\n",
    "        early_stopping_patience_epochs: int = 0,  # 0 to disable early stopping feature\n",
    "        precision=32,\n",
    "        logger=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        trains T5/MT5 model on custom dataset\n",
    "        Args:\n",
    "            train_df (pd.DataFrame): training datarame. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n",
    "            eval_df ([type], optional): validation datarame. Dataframe must have 2 column --> \"source_text\" and \"target_text\"\n",
    "            source_max_token_len (int, optional): max token length of source text. Defaults to 512.\n",
    "            target_max_token_len (int, optional): max token length of target text. Defaults to 512.\n",
    "            batch_size (int, optional): batch size. Defaults to 8.\n",
    "            max_epochs (int, optional): max number of epochs. Defaults to 5.\n",
    "            use_gpu (bool, optional): if True, model uses gpu for training. Defaults to True.\n",
    "            outputdir (str, optional): output directory to save model checkpoints. Defaults to \"outputs\".\n",
    "            early_stopping_patience_epochs (int, optional): monitors val_loss on epoch end and stops training, if val_loss does not improve after the specied number of epochs. set 0 to disable early stopping. Defaults to 0 (disabled)\n",
    "            precision (int, optional): sets precision training - Double precision (64), full precision (32) or half precision (16). Defaults to 32.\n",
    "            logger (pytorch_lightning.loggers.base.LightningLoggerBase, optional): A logger object to pass in to the Trainer. Defaults to None (disabled).\n",
    "        \"\"\"\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "        self.data_module = LightningDataModule(\n",
    "            train_df,\n",
    "            eval_df,\n",
    "            self.tokenizer,\n",
    "            batch_size=batch_size,\n",
    "            source_max_token_len=source_max_token_len,\n",
    "            target_max_token_len=target_max_token_len,\n",
    "        )\n",
    "\n",
    "        self.T5Model = LightningModel(\n",
    "            tokenizer=self.tokenizer, model=self.model, outputdir=outputdir\n",
    "        )\n",
    "\n",
    "        # checkpoint_callback = ModelCheckpoint(\n",
    "        #     dirpath=\"checkpoints\",\n",
    "        #     filename=\"best-checkpoint-{epoch}-{train_loss:.2f}\",\n",
    "        #     save_top_k=-1,\n",
    "        #     verbose=True,\n",
    "        #     monitor=\"train_loss\",\n",
    "        #     mode=\"min\",\n",
    "        # )\n",
    "\n",
    "        # logger = TensorBoardLogger(\"SimpleT5\", name=\"SimpleT5-Logger\")\n",
    "\n",
    "        early_stop_callback = (\n",
    "            [\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    min_delta=0.00,\n",
    "                    patience=early_stopping_patience_epochs,\n",
    "                    verbose=True,\n",
    "                    mode=\"min\",\n",
    "                )\n",
    "            ]\n",
    "            if early_stopping_patience_epochs > 0\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        gpus = 1 if use_gpu else 0\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            logger=logger,\n",
    "            callbacks=early_stop_callback,\n",
    "            max_epochs=max_epochs,\n",
    "            gpus=gpus,\n",
    "            progress_bar_refresh_rate=5,\n",
    "            precision=precision,\n",
    "        )\n",
    "\n",
    "        trainer.fit(self.T5Model, self.data_module)\n",
    "\n",
    "    def load_model(\n",
    "        self, model_type: str = \"t5\", model_dir: str = \"outputs\", use_gpu: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        loads a checkpoint for inferencing/prediction\n",
    "        Args:\n",
    "            model_type (str, optional): \"t5\" or \"mt5\". Defaults to \"t5\".\n",
    "            model_dir (str, optional): path to model directory. Defaults to \"outputs\".\n",
    "            use_gpu (bool, optional): if True, model uses gpu for inferencing/prediction. Defaults to True.\n",
    "        \"\"\"\n",
    "        if model_type == \"t5\":\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(f\"{model_dir}\")\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(f\"{model_dir}\")\n",
    "        elif model_type == \"mt5\":\n",
    "            self.model = MT5ForConditionalGeneration.from_pretrained(f\"{model_dir}\")\n",
    "            self.tokenizer = MT5Tokenizer.from_pretrained(f\"{model_dir}\")\n",
    "        elif model_type == \"byt5\":\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(f\"{model_dir}\")\n",
    "            self.tokenizer = ByT5Tokenizer.from_pretrained(f\"{model_dir}\")\n",
    "\n",
    "        if use_gpu:\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(\"cuda\")\n",
    "            else:\n",
    "                raise \"exception ---> no gpu found. set use_gpu=False, to use CPU\"\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 512,\n",
    "        num_return_sequences: int = 1,\n",
    "        num_beams: int = 2,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.95,\n",
    "        do_sample: bool = True,\n",
    "        repetition_penalty: float = 2.5,\n",
    "        length_penalty: float = 1.0,\n",
    "        early_stopping: bool = True,\n",
    "        skip_special_tokens: bool = True,\n",
    "        clean_up_tokenization_spaces: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        generates prediction for T5/MT5 model\n",
    "        Args:\n",
    "            source_text (str): any text for generating predictions\n",
    "            max_length (int, optional): max token length of prediction. Defaults to 512.\n",
    "            num_return_sequences (int, optional): number of predictions to be returned. Defaults to 1.\n",
    "            num_beams (int, optional): number of beams. Defaults to 2.\n",
    "            top_k (int, optional): Defaults to 50.\n",
    "            top_p (float, optional): Defaults to 0.95.\n",
    "            do_sample (bool, optional): Defaults to True.\n",
    "            repetition_penalty (float, optional): Defaults to 2.5.\n",
    "            length_penalty (float, optional): Defaults to 1.0.\n",
    "            early_stopping (bool, optional): Defaults to True.\n",
    "            skip_special_tokens (bool, optional): Defaults to True.\n",
    "            clean_up_tokenization_spaces (bool, optional): Defaults to True.\n",
    "        Returns:\n",
    "            list[str]: returns predictions\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            source_text, return_tensors=\"pt\", add_special_tokens=True\n",
    "        )\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            num_beams=num_beams,\n",
    "            max_length=max_length,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            early_stopping=early_stopping,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "        )\n",
    "        preds = [\n",
    "            self.tokenizer.decode(\n",
    "                g,\n",
    "                skip_special_tokens=skip_special_tokens,\n",
    "                clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "            )\n",
    "            for g in generated_ids\n",
    "        ]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "#from simplet5 import SimpleT5\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(project = 'bakbak', entity = 'quarter100',name=f'mtsmall')\n",
    "model = SimpleT5()\n",
    "model.from_pretrained(model_type=\"mt5\", model_name=\"google/mt5-small\")\n",
    "model.train(train_df=train_df,\n",
    "            eval_df=test_df, \n",
    "            source_max_token_len=512, # default\n",
    "            target_max_token_len=512, # defalut\n",
    "            batch_size=8, max_epochs=100, early_stopping_patience_epochs = 3, use_gpu=True, logger = wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7160667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "from simplet5 import SimpleT5\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# load json\n",
    "def json_load(data_path = '/opt/ml/data/요약대회/test_summary.json'):\n",
    "    with open(data_path, 'r') as json_file:\n",
    "            test_json = json.load(json_file)\n",
    "    return test_json\n",
    "\n",
    "# model\n",
    "model = SimpleT5()\n",
    "model.load_model(\"mt5\",\"./outputs/simplet5-epoch-8-train-loss-0.3569\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "421bd968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:35<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "test_json = json_load()\n",
    "\n",
    "for i in tqdm(range(20)):#len(test_json)):\n",
    "        test_json[i]['summary'] = model.predict(test_json[i]['original'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c07195a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'original': '공주시 무령왕릉에서 발견된 청동거울로 청동신수경, 의자손수대경, 수대경 3점이다. 청동신수경은 ‘방격규구문경’이라는 중국 후한의 거울을 모방하여 만든 것이다. 거울 내부에는 반나체 인물상과 글이 새겨져 있는데 이는 한나라의 거울에서 흔히 볼 수 있는 것이다. 의자손수대경은 중국 한대의 수대경을 본떠 만든 방제경이다. 거울 중앙의 꼭지를 중심으로 9개의 돌기가 있고, 안에는 크고 작은 원과 7개의 돌기가 솟아있다. 내부 주위의 테두리에는 명문이 새겨져 있으나 선명하지 못하여 알아볼 수 없다. 수대경 역시 한나라 때 동물 문양을 새겨 넣은 수대경을 본떠서 만들어진 방제경이다. 그러나 한나라 거울에 비해 선이 굵고 무늬가 정교하지 못하다.',\n",
       "  'summary': '청동신수경은 ‘방격규구문경’이라는 중국 후한의 거울을 모방하여 만든 것이다. 거울 내부에는 반나체 인물상과 글이 새겨져 있는데 이는 한나라의 거울에서 흔히 볼 수 있는 것이다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-00164-01180',\n",
       "   'doc_name': '무령왕릉 청동거울 일괄 (武寧王陵 銅鏡 一括)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '높이 47cm의 대형 백자 병으로 목이 유난히 길어 속칭 거위병이라고도 부른다. 병은 몸통 부분에서 목이 길고 가늘게 올라가서 구연부에서 둥글게 말리는 모습이다. 긴 목에 비해 몸통이 다소 왜소해 보여 조형상의 균형감이 다소 떨어지지만, 두툼하게 제작된 하단부가 무게중심을 이루어 안정감이 있다. 잡물이 조금 포함된 백토에 투명유약을 시유하여 광택이 돈다. 백자를 굽는 과정에서 생긴 잔 빙렬(표면의 유약층에 작게 반복적으로 금이 간 상태)이 있으며 병에 담았던 액체가 하단부에 스미어 나와 흔적으로 남았지만, 은은한 순백색의 유면은 따뜻한 느낌을 주고 있어 정치·경제적으로 부강했던 18세기 조선의 당당한 위상을 보여 주는 듯 하다. 경기도자박물관 소장 백자대병의 용도는 다병(茶甁;차를 담는 병)이었을 것으로 보인다.',\n",
       "  'summary': '긴 목에 비해 몸통이 다소 왜소해 보여 조형상의 균형감이 다소 떨어지지만, 두툼하게 제작된 하단부가 무게중심을 이루어 안정감이 있다. 잡물이 조금 포함된 백토에 투명유약을 시유하여 광택이 돈다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-07027-00492',\n",
       "   'doc_name': '백자 대병 (白磁 大甁)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '공주시 무령왕릉에서 출토된 백제 때 귀고리 2쌍으로 길이는 11.8㎝, 8.8㎝이다. 왕비의 귀고리로, 굵은 고리를 중심으로 작은 장식들을 연결하여 만들었다. 한 쌍은 복잡한 형식으로 길고 짧은 2줄의 장식이 달려 있고, 다른 한 쌍은 1줄로만 되어있다. 앞의 귀고리 중 긴 가닥은 금 철사를 꼬아서 만든 사슬에 둥근 장식을 많이 연결하였으며, 맨 밑에는 작은 고리를 연결하여 8개의 둥근 장식을 달고 그 아래 탄환 모양의 장식을 달았다. 짧은 줄의 수식은 다른 한 쌍의 것과 거의 같은 수법이나 탄환 장식은 달지 않고, 잎사귀 모양의 장식과 담록색의 둥근 옥을 달았다. 국립중앙박물관과 국립공주박물관에 각1쌍씩 보관되어 있다.',\n",
       "  'summary': '왕비의 귀고리로, 굵은 고리를 중심으로 작은 장식들을 연결하여 만들었다. 한 쌍은 복잡한 형식으로 길고 짧은 2줄의 장식이 달려 있고, 다른 한 쌍은 1줄로만 되어있다. 앞의 귀고리 중 긴 가닥은 금 철사를 꼬아서 만든 사슬에 둥근 장식을 많이 연결하였으며, 맨 밑에는 작은 고리를 연결하여 8개의 둥근 장식을 달고 그 아래 탄환 모양의 장식을 달았다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-00169-00647',\n",
       "   'doc_name': '무령왕비 금귀걸이 (武寧王妃 金製耳飾)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '해우소(解憂所)는 ‘근심을 해결하는 장소’ 라는 뜻의 사찰에서 화장실을 이르는 말이다. 전통적 형식을 지닌 보덕사 해우소는 앞면 3칸, 옆면 1칸 규모로 맞배지붕을 하고 있는 2층 누각식 건물이다. 앞뒤 2열로 나누어 각각 6칸씩의 대변소를 배치하여 남녀의 사용을 구분하면서 12명을 동시에 수용할 수 있는 지혜를 엿볼 수 있는 건물이다. 상량문을 통해 조선 고종 19년(1882)에 세운 건물임을 알 수 있는 이 해우소는 세워진 지 오래 되었음에도 원형을 잘 유지하고 있으며, 오래된 사찰 해우소 건물로는 강원도내에서는 희소가치를 지니고 있다.',\n",
       "  'summary': '앞뒤 2열로 나누어 각각 6칸씩의 대변소를 배치하여 남녀의 사용을 구분하면서 12명을 동시에 수용할 수 있는 지혜를 엿볼 수 있는 건물이다. 상량문을 통해 조선 고종 19년(1882)에 세운 건물임을 알 수 있는 이 해우소는 세워진 지 오래 되었음에도 원형을 잘 유지하고 있으며, 오래된 사찰 해우소 건물로는 강원도내에서는 희소가치를 지니고 있다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-11866-01193',\n",
       "   'doc_name': '영월보덕사해우소 (寧越報德寺解憂所)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '완도군 완도읍 군내리 신흥사에 모셔진 약사여래좌상이다. 이 불상은 원래 해남 대흥사 소속암자인 심적암(深寂庵)에 있었던 것인데 초의스님이 현 대광명전에 옮겨 모셨으며, 그 뒤 응송(박영희)스님이 신흥사로 옮겨 봉안한 것이다. 불상에서 나온 복장물의 발원문에 의하면 불상 명칭은 약사여래좌상으로 호칭되었고 1628년에 처음 조성하였으며 1802년 중수, 1845년 개금불사, 1865년 중수개금을 하였음을 알 수 있다. 이 불상의 조성연대는 1628년으로, 임진왜란의 혼란기를 지나서 서서히 불상조성이 다시 살아나는 시기의 불상이라는 점, 그리고 이 불상의 양식이 아직까지는 조선전기의 전통을 계승하고 있음을 엿볼 수 있으며 이러한 양식이 조선후기, 즉 18∼19세기 불상에 어떤 양식으로 변천해 가는가를 연구하는데 하나의 표준이 되고 있다. 조선시대 불상으로 그 조성연대를 알 수 있어 불상 편년사 연구에 중요한 자료로 평가된다.',\n",
       "  'summary': '이 불상의 조성연대는 1628년으로, 임진왜란의 혼란기를 지나서 서서히 불상조성이 다시 살아나는 시기의 불상이라는 점, 그리고 이 불상의 양식이 아직까지는 조선전기의 전통을 계승하고 있음을 엿볼 수 있으며 이러한 양식이 조선후기, 즉 18∼19세기 불상에 어떤 양식으로 변천해 가는가를 연구하는데 하나의 표준이 되고 있다. 조선시대 불상으로 그 조성연대를 알 수 있어 불상 편년사 연구에 중요한 자료로 평가된다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-12296-00290',\n",
       "   'doc_name': '완도신흥사목조약사여래좌상 (莞島新興寺木造藥師如來坐像)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '농악은 농부들이 두레를 짜서 일할 때 치는 음악으로 꽹과리·징·장구·북과 같은 타악기를 치며 벌이는 음악을 두루 가리키는 말이다. 농악을 공연하는 목적에 따라 종류를 나누어 보면 당산굿·마당밟이·걸립굿·두레굿·판굿·기우제굿·배굿 등으로 나눌 수 있고, 지역적 특성에 따라 분류하면 경기농악·영동농악·호남우도농악·호남좌도농악·경남농악·경북농악으로 갈라진다. 경남농악의 한 종류인 부산농악은 음력 1월 초에 집집마다 돌아가면서 농악을 치고 고사를 지내며 복을 빌어주는 걸립굿이 주류를 이룬다. 다른 농악에 비해 느린 4박자로 굿거리장단이 많고 춤이 많이 들어가 있다. 상모돌리기, 버꾸놀이의 기능이 뛰어나며 특히 4개의 북이 일치되어 북을 안고 넘는 기교가 일품이다.민족의 역사와 더불어 이어져 온 이 농악은 우리 민족의 오랜 토속신앙이며, 춤과 장단이 어우러진 전통적인 민속예술로 보존되어야 할 것이다. 현재 부산농악보존협회에서 전승·보급에 힘쓰고 있다.',\n",
       "  'summary': '경남농악의 한 종류인 부산농악은 음력 1월 초에 집집마다 돌아가면서 농악을 치고 고사를 지내며 복을 빌어주는 걸립굿이 주류를 이룬다. 다른 농악에 비해 느린 4박자로 굿거리장단이 많고 춤이 많이 들어가 있다. 상모돌리기, 버꾸놀이의 기능이 뛰어나며 특히 4개의 북이 일치되어 북을 안고 넘는 기교가 일품이다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-07981-03384',\n",
       "   'doc_name': '부산농악 (釜山農樂)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '화기에 의한 1868년 3월 서운암(瑞雲菴)에 봉안한다고 적혀있으나, 밑부분이 잘려 신중도를 그린 불화승은 알 수 없음. 그러나 거의 같은 초본을 사용한 신중도를 통해 서운암의 위치를 추적해 볼 수 있음. 선학원 신중도와 같은 형식은 해인사 국일암 신중도(1885년) 직지사 삼성암 신중도(1888년), 통도사 말사인 성전암 신중도(19세기) 등이 전해짐. 특히 국일암 신중도와 성전암 신중도와는 초본이 거의 같은 것으로 보아, 이 선학원 신중도의 서운암은 경상도 지역의 사찰로 생각됨. 선학원 신중도는 각 존상의 얼굴 표정 묘사가 돋보이는 등 그 형태나 필선, 구도, 문양 등에서 질적으로 뛰어나며, 19세기 후반 경상도 지역에서 유사한 모본으로 그려진 신중도 중 가장 연대가 올라가는 작품임.',\n",
       "  'summary': '선학원 신중도와 같은 형식은 해인사 국일암 신중도(1885년) 직지사 삼성암 신중도(1888년), 통도사 말사인 성전암 신중도(19세기) 등이 전해짐. 특히 국일암 신중도와 성전암 신중도와는 초본이 거의 같은 것으로 보아, 이 선학원 신중도의 서운암은 경상도 지역의 사찰로 생각됨.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-07350-01074',\n",
       "   'doc_name': '선학원 신중도 (禪學院 神衆圖)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '법주사내의 보광명전 앞에 서 있는 탑으로, 원래는 2층 기단(基壇) 위에 5층의 탑신(塔身)을 올린 모습이었을 것이나, 현재는 위층 기단과 탑신의 4층 몸돌·지붕돌이 없는 상태이다. 탑신은 1층 몸돌에 비해 2층 몸돌이 급격히 줄어들었고, 그 이상의 몸돌은 크기가 거의 줄지 않았다. 1·2·3층 몸돌에는 모서리마다 기둥 모양을 새겼고, 특히 1층 몸돌 남쪽면에는 문모양과 글씨를 새겨 놓았다. 지붕돌은 낙수면이 깊이 패이고 네 귀퉁이가 살짝 치켜올라가 우아한 곡선을 그린다. 그러나 5층만은 몸돌에 기둥 모양을 새기지 않았고, 지붕돌의 곡선도 밋밋하다. 꼭대기에는 머리장식들을 차례대로 얹어 놓았다. 고려시대에 세운 탑으로, 일부 석재를 잃어버려 본래의 모습을 알 수 없음이 아쉬울 따름이다.',\n",
       "  'summary': '고려시대에 세운 탑으로, 일부 석재를 잃어버려 본래의 모습을 알 수 없음이 아쉬울 따름이다. 고려시대에 세운 탑으로, 일부 석재를 잃어버려 본래의 모습을 알 수 없음이 아쉬울 따름이다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-11025-01102',\n",
       "   'doc_name': '법주사오층석탑 (法住寺五層石塔)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '신라 김유신(595∼673)장군의 위패를 모신 사당이다.  신라가 삼국을 통일한 후에 변방지역에 말갈족이 자꾸 침입하여 괴롭히자, 명주(지금의 강릉)의 화부산 밑에 장군이 머물면서 적을 퇴치하여 평화를 되찾았다고 한다. 그 후 장군이 세상을 떠나자 백성들이 조선 고종 21년(1884) 화부산의 밑에 사당을 짓고 제사를 지내며 장군을 추모하여 오다가, 1936년 강릉역 확장으로 인하여 지금의 위치로 옮겨졌다.  앞면 3칸·옆면 2칸 규모이며, 지붕 옆면이 사람 인(人)자 모양인 맞배지붕집이다. 경내에는 장군의 생애와 공적이 적힌 ‘순충장열무흥왕화산제기적비(純忠壯烈武興王花山薺紀蹟碑)’가 있다.',\n",
       "  'summary': '신라가 삼국을 통일한 후에 변방지역에 말갈족이 자꾸 침입하여 괴롭히자, 명주(지금의 강릉)의 화부산 밑에 장군이 머물면서 적을 퇴치하여 평화를 되찾았다고 한다. 그 후 장군이 세상을 떠나자 백성들이 조선 고종 21년(1884) 화부산의 밑에 사당을 짓고 제사를 지내며 장군을 추모하여 오다가, 1936년 강릉역 확장으로 인하여 지금의 위치로 옮겨졌다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-04839-01847',\n",
       "   'doc_name': '화부산사 (花浮山祠)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '지석묘는 청동기시대의 대표적인 무덤으로 고인돌이라고도 부르며, 주로 경제력이 있거나 정치권력을 가진 지배층의 무덤으로 알려져 있다. 우리나라의 고인돌은 4개의 받침돌을 세워서 돌방을 만들고 그 위에 거대하고 평평한 덮개돌을 올려놓은 탁자식과, 땅 속에 돌방을 만들고 작은 받침돌을 세운 뒤 뚜껑돌을 덮고 그 위에 거대한 덮개돌을 올린 바둑판식으로 구분된다. 김해시 서쪽 경운산 기슭의 마을 안에 있는데, 원래는 더 많은 고인돌이 있었던 것 같으나 주택개축과 개간 등으로 유실되어 현재는 1기만 남아 있다. 형태상 바둑판식으로 덮개돌의 크기는 길이 3.1m, 너비 2.4m이며, 땅 속에 돌을 쌓아 만든 석곽을 설치하였다. 고인돌 주변에서 붉은간토기, 민무늬토기 등이 출토되어 청동기시대의 생활상을 연구하는데 중요한 자료로 평가된다.',\n",
       "  'summary': '우리나라의 고인돌은 4개의 받침돌을 세워서 돌방을 만들고 그 위에 거대하고 평평한 덮개돌을 올려놓은 탁자식과, 땅 속에 돌방을 만들고 작은 받침돌을 세운 뒤 뚜껑돌을 덮고 그 위에 거대한 덮개돌을 올린 바둑판식으로 구분된다. 형태상 바둑판식으로 덮개돌의 크기는 길이 3.1m, 너비 2.4m이며, 땅 속에 돌을 쌓아 만든 석곽을 설치하였다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-09469-03085',\n",
       "   'doc_name': '김해 내동지석묘 (金海 內洞支石墓)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '고려 원종 원년(1260) 대원사를 크게 중창한 바 있고, 한때 이곳에 머물렀던 자진국사의 사리탑이다. 자진국사는 고려 고종 2년(1215)에 태어나 충열왕 12년(1286) 72세로 입적할 때까지 조계산 송광사에서 그 이름을 떨쳤다.  그의 사리탑은 대원사 극락전 오른쪽에 자리하고 있으며, 기단부(基壇部)·탑신(塔身) 등이 모두 8각을 이루고 있다. 기단부는 연꽃무늬를 새긴 아래받침돌 위로, 활짝핀 연꽃모양의 윗받침돌이 바로 놓여 있는데, 가운데받침돌을 생략한 것인지 잃어버린 것인지는 정확히 알 수 없다. 탑신의 몸돌은 비교적 높고 가늘고, 두텁고 좁은 지붕돌은 급한 경사가 흐르며 여덟 귀퉁이에서의 치켜올림이 약하다. 꼭대기에는 복발(覆鉢:얹어놓은 대접모양의 장식), 보주(寶珠:꽃봉오리 모양 장식)가 얹혀져 머리장식을 하고 있는데, 훗날에 새로 만든 것으로 보인다. 전체적으로 섬세한 조각수법은 보이지 않으나, 단순하면서도 세련된 고려시대의 사리탑이라 할 수 있다.',\n",
       "  'summary': '전체적으로 섬세한 조각수법은 보이지 않으나, 단순하면서도 세련된 고려시대의 사리탑이라 할 수 있다. 전체적으로 섬세한 조각수법은 보이지 않으나, 단순하면서도 세련된 고려시대의 사리탑이라 할 수 있다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-04615-03162',\n",
       "   'doc_name': '대원사자진국사부도 (大原寺慈眞國師浮屠)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '전라남도 광양시 옥룡면에 있는 산성으로, 6개의 산봉우리를 아우르며 성 안으로 계곡을 품고 있다. 이곳은 백운산 중턱의 한재를 중심으로 구례·남원·하동·화개로 통하는 교통의 요충지이다. 산 전체가 천연적인 요새로 흙을 쌓아 만든 토성인데, 단순히 흙을 쌓아올린 것이 아니고, 일정한 두께로 흙을 다져 쌓았다. 외성은 길이가 약 4㎞ 정도로 매우 길며, 외성 안쪽에 약 240m에 이르는 내성을 흙으로 쌓았다. 중흥사 입구 세심정에 남문, 옥룡면 추산리로 넘어가는 오솔길에 북문터가 남아 있다. 임진왜란 때는 의병과 승병을 양성하는 훈련장으로 사용하였으며, 의병과 승병으로 구성된 연합군과 왜군간에 큰 전투가 벌어졌던 곳이다. 광양시에서 조사된 산성 가운데 유일한 토성으로, 만든 시기도 고려시대로 추정하고 있어 보존가치가 높은 곳이다.',\n",
       "  'summary': '임진왜란 때는 의병과 승병을 양성하는 훈련장으로 사용하였으며, 의병과 승병으로 구성된 연합군과 왜군간에 큰 전투가 벌어졌던 곳이다. 임진왜란 때는 의병과 승병을 양성하는 훈련장으로 사용하였으며, 의병과 승병으로 구성된 연합군과 왜군간에 큰 전투가 벌어졌던 곳이다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-09935-02471',\n",
       "   'doc_name': '광양중흥산성 (光陽中興山城)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '보광사 목조지장보살좌상은 1654년 내시 나업의 부인 한씨가 죽은 남편을 위해 금강산 안양암에 봉안하기 위하여 발원한 작품으로, 조각승 초안이 제작한 기년명 불상이다. 목조지장보살좌상은 하나의 나무로 만들어진 불상이다. 지장보살은 높이가 46㎝, 무릎 너비가 30.5㎝로. 높이와 폭이 1:0.6의 신체비례를 가져 17세기 중반에 제작된 기년명 보살상에 비하여 옆으로 퍼진 느낌이 든다.불상 내에서 발견된 복장물 가운데 조성발원문, 불교경전 등은 조선시대 불교사와 정치사 연구에 중요한 문헌자료들이다. 특히 복장물 중에 [제불명칭가곡]은 15세기 명나라에서 조선 왕실에 보낸 경전으로, 조선왕조실록에 여러 번 언급되었다.',\n",
       "  'summary': '목조지장보살좌상은 1654년 내시 나업의 부인 한씨가 죽은 남편을 위해 금강산 안양암에 봉안하기 위하여 발원한 작품으로, 조각승 초안이 제작한 기년명 불상이다. 목조지장보살좌상은 하나의 나무로 만들어진 불상이다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-05984-01427',\n",
       "   'doc_name': '속초 보광사 목조지장보살좌상과 복장유물 (束草 普光寺 木造地藏菩薩坐像과 腹藏遺物)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '이 교회는 전라남도 지역이 광주교구로 승격되던 1937년 광주성당의 4대 주임신부로 부임한 구 具 신부(Rev. Thomas Quinlan)가 계획을 세워서 지은 것이다. 구신부는 교회를 신축하기 위해서 성당건축에 경험이 많은 중국인 가요셉에게 설계와 공사를 맡겨서 1937년 10월에 착공하여 이듬해 6월에 낙성하였다. 건물의 평면은 가늘고 긴 사각형으로 정면에 종탑을 배치하고 좌측면에만 제의실과 고해실을 배치하였다. 외벽은 고막이돌을 놓고 붉은색 벽돌을 쌓았으며, 종탑과 측면에 있는 창틀과 문에는 화강석으로 인방을 설치하였다. 내부의 벽과 천정은 별다른 장식이 없이 흰색 회를 발라 마감하였다. 1987년 7월에 일부를 증축하였고, 창틀과 지붕은 최근에 재료를 바꾸어 보수하였다.',\n",
       "  'summary': '구신부는 교회를 신축하기 위해서 성당건축에 경험이 많은 중국인 가요셉에게 설계와 공사를 맡겨서 1937년 10월에 착공하여 이듬해 6월에 낙성하였다. 건물의 평면은 가늘고 긴 사각형으로 정면에 종탑을 배치하고 좌측면에만 제의실과 고해실을 배치하였다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-08829-02465',\n",
       "   'doc_name': '광주북동천주교회 (광주북동천주교회)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '『중용주자혹문(中庸朱子或問)』은 고려 공민왕 20년(1371)에 원나라로부터 수입하여 목판본으로 찍어낸 책으로, 송나라 주희가『사서(四書)』가운데 하나인『중용』에 대해 여러 문제점을 묻고 답하는 형식으로 서술한 책이다. 비교적 원본(元本)의 특징이 잘 나타나 있으며, 글자새김이 정교하고 인쇄가 깨끗한 편이다. 현재 두 권이 전해지는데 한 권은 고려대학교도서관에서, 다른 한 권은 서울에 사는 조병순씨가 소장하고 있으며, 각각 보물 제706호 ·보물 제707호로 지정되어 있다.  또한 책 끝에 홍호(洪虎) 4년(1371) 7월에 진주목에서 개판(開板)하였다는 기록이 있어 이는 고려시대 지방관판본으로 문헌학 연구에 귀중한 자료가 된다. 홍호(洪虎)는 곧 홍무(洪武)이다. 고려 혜종(惠宗)의 이름이 무(武)이므로 피휘(避諱)하여 바꾸어 쓴 것이다. 이런 예는 고려간본(高麗刊本)에서 흔히 볼 수 있다.인쇄상태로 보아 이 책이 보물 제706호보다 약간 늦게 인쇄된 듯하다.',\n",
       "  'summary': '현재 두 권이 전해지는데 한 권은 고려대학교도서관에서, 다른 한 권은 서울에 사는 조병순씨가 소장하고 있으며, 각각 보물 제706호 ·보물 제707호로 지정되어 있다. 비교적 원본(元本)의 특징이 잘 나타나 있으며, 글자새김이 정교하고 인쇄가 깨끗한 편이다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-01076-03753',\n",
       "   'doc_name': '중용주자혹문 (中庸朱子或問)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '전라북도 임실군 삼계면 학정리의 밭 가운데에 서 있는 불상으로, 하반신이 땅속에 묻혀 있다. 땅위로 드러난 부분의 높이는 1.55m, 너비는 0.85m이다. 민머리 위에는 상투 모양의 머리묶음이 큼직하게 표현되어 있다. 풍만한 얼굴에 눈은 양 끝이 치켜 올라가 있고, 입은 작은 편이다. 양 어깨를 두른 옷은 가슴에서 완만한 U자형을 이루고 있으며, 옷주름은 선이 희미해서 형식화된 듯하다. 가슴 아래가 땅속에 묻혀 하체의 형태를 확인할 수 없다. 왼손이 배 중앙을 지나 오른손 관절부분에 이르고 있는데, 약그릇 같은 것을 들고 있는 듯한 모습이어서 약사여래불로 추정하고 있다.',\n",
       "  'summary': '왼손이 배 중앙을 지나 오른손 관절부분에 이르고 있는데, 약그릇 같은 것을 들고 있는 듯한 모습이어서 약사여래불로 추정하고 있다. 민머리 위에는 상투 모양의 머리묶음이 큼직하게 표현되어 있다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-05117-04209',\n",
       "   'doc_name': '학정리석불 (鶴亭里石佛)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '임진왜란 때 참전하였다가 숨진 박천붕(1545∼1592)과 선친의 위업을 이어 병자호란 때 참전하였다가 순직한 네 아들 원겸·인겸·례겸·의겸의 충절을 기리기 위해 나라에서 세운 비각이다. 박천붕은 임진왜란 때 의병장으로 활약했던 조헌의 종사관으로 승병 영규대사와 함께 분전한 청주전투에서 전사하였다. 영조 24년(1748)에 세운 이 정각은 앞면 3칸·옆면 1칸 규모로 안에는 신구비(新舊碑) 2구가 있다. 구비(舊碑)는 고종 23년(1886)년 화재로 소실되었던 건물을 다시 지으며 세운 것으로 앞면에는 ‘밀양박씨오충기실비’란 글씨가 해서체로 써 있고 비의 뒷면에는 정려비에 대한 내용이 있었으나, 1938년 일본경찰이 훼손하여 지금은 아무 것도 남아 있지 않다. 또 다른 비석에는 5부자의 순절 내용이 기록되어 있다.',\n",
       "  'summary': '박천붕은 임진왜란 때 의병장으로 활약했던 조헌의 종사관으로 승병 영규대사와 함께 분전한 청주전투에서 전사하였다. 영조 24년(1748)에 세운 이 정각은 앞면 3칸·옆면 1칸 규모로 안에는 신구비(新舊碑) 2구가 있다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-08691-00924',\n",
       "   'doc_name': '담양오충정려 (潭陽五忠旌閭)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '현수제승법수(賢首諸乘法數)지정번호 : 경기도유형문화재 제219호시 대 : 1500년(조선 연산군 5년)현수제승법수는 불경에 나오는 숫자가 들어 있는 용어를 가려 뽑아서 숫자의 차례대로 배열하여 쉽게 찾아볼 숭 있도록 엮어놓은 일종의 불교용어사전이다. 본래 이 책은 당나라 승려 현수(賢首)가 부조의 설법을 숫자로서 표시하여 일목요연하게 이해할 수 있도록 편찬했는데, 후대 오면서 점차 희귀해지자 명나라 행심(行深)이 불교입문의 지침이 될 만한 체재로 재편하였다. 이 목판본은 합천 봉서사(鳳栖寺)에서 간행한 것으로 11권 1책이며, 서문과 발문에서 편찬 취지와 간행 동기를 명확하게 밝히고 있어 불교문화재로서 뿐만 아니라 서지학적으로 매우 귀중한 자료이다.',\n",
       "  'summary': '이 목판본은 합천 봉서사(鳳栖寺)에서 간행한 것으로 11권 1책이며, 서문과 발문에서 편찬 취지와 간행 동기를 명확하게 밝히고 있어 불교문화재로서 뿐만 아니라 서지학적으로 매우 귀중한 자료이다. 본래 이 책은 당나라 승려 현수(賢首)가 부조의 설법을 숫자로서 표시하여 일목요연하게 이해할 수 있도록 편찬했는데, 후대 오면서 점차 희귀해지자 명나라 행심(行深)이 불교입문의 지침이 될 만한 체재로 재편하였다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-06239-01287',\n",
       "   'doc_name': '현수제승법수 (賢首諸乘法數)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '대구 보성선원 목조석가여래삼존좌상 복장불서는 보성선원에 봉안되어 있는 본존불인 석가여래, 좌우 문수ㆍ보현보살상(보물 제1801호)에 납입되었던 서적이다. 불상은 조성발원문을 통해 1647년(인조 27) 조각승 玄旭이 조성한 것으로 판명되어 2013년 보물로 지정되었다. 아울러 복장되었던 발원문 등 복장전적이 동시에 보물 제1802호로 지정되었다. 당시에 보물로 지정되지 못한 불경도 모두 1635년(인조 13)년 이전에 인출되었다. 모두 불상조성(1647)보다 앞서 병자호란(1636) 이전에 인출되었으므로 일괄해서 유형문화재로 지정해서 지정해 보존할 가치가 있다.',\n",
       "  'summary': '불상은 조성발원문을 통해 1647년(인조 27) 조각승 玄旭이 조성한 것으로 판명되어 2013년 보물로 지정되었다. 아울러 복장되었던 발원문 등 복장전적이 동시에 보물 제1802호로 지정되었다. 당시에 보물로 지정되지 못한 불경도 모두 1635년(인조 13)년 이전에 인출되었다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-05028-00082',\n",
       "   'doc_name': '대구 보성선원 목조석가여래삼존좌상 복장불서 (大邱 寶聖禪院 木造釋迦如來三尊坐像 腹藏佛書)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}},\n",
       " {'original': '개목사의 원래 이름은 흥국사로 통일신라 신문왕(재위 681∼692) 때 의상대사가 세웠다고 한다. 전설에 따르면 의상대사가 절 뒤에 있는 천등굴에서 천녀(天女)의 기적으로 도를 깨치고 절을 세웠다고 하는데 ‘개목사’라는 절 이름은 조선시대에 바꾼 이름이라고 전한다. 관세음보살을 모셔 놓은 원통전은 1969년 해체·수리시 발견한 기록에 ‘천순원년(天順元年)’이라는 글귀가 있어 세조 3년(1457)에 지은 것으로 짐작한다. 규모는 앞면과 옆면이 3칸씩이며, 지붕은 옆면에서 볼 때 사람 인(人)자 모양을 한 맞배지붕이다. 지붕 무게를 받치기 위해 기둥 윗부분에 간결하게 짜은 구조가 기둥 위에만 있는 주심포 양식이다. 건물 안쪽 천장은 뼈대가 보이는 연등천장이고 법당 안을 온돌방으로 만들어 놓아 조선 전기 건물로는 보기 드문 예가 되고 있다.  전체적으로 조선 전기 건축양식의 특징을 잘 간직하고 있어 건축사 연구에 소중한 자료로 평가받고 있다.',\n",
       "  'summary': '전체적으로 조선 전기 건축양식의 특징을 잘 간직하고 있어 건축사 연구에 소중한 자료로 평가받고 있다. 관세음보살을 모셔 놓은 원통전은 1969년 해체·수리시 발견한 기록에 ‘천순원년(天順元年)’이라는 글귀가 있어 세조 3년(1457)에 지은 것으로 짐작한다.',\n",
       "  'Meta': {'passage_id': 'REPORT-cultural_assets-00582-00322',\n",
       "   'doc_name': '안동 개목사 원통전 (安東 開目寺 圓通殿)',\n",
       "   'category': 'cul_ass',\n",
       "   'author': None,\n",
       "   'publisher': None,\n",
       "   'publisher_year': None,\n",
       "   'doc_origin': '문화재청'}}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_json[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da5f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
